{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tidy Data in Python\n",
    "by [Jean-Nicholas Hould](http://www.jeannicholashould.com/)\n",
    "\n",
    "\n",
    "from the blog post of the same name\n",
    "[http://www.jeannicholashould.com/tidy-data-in-python.html](http://www.jeannicholashould.com/tidy-data-in-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying messy datasets (Intro)\n",
    "\n",
    "Two very common problems with messy datasets are:\n",
    "\n",
    "- A column contains multiple-valued lists\n",
    "- Column headers are values, not variable names\n",
    "\n",
    "We'll run through how to fix these problems in the examples below. We use the Python module `Pandas` for dealing with tablular data.\n",
    "\n",
    "We'll also learn how to join two tables, both by concatenation, and by the equivalent of an SQL JOIN statment in Pandas.\n",
    "\n",
    "Plus, we'll see how to save tables to CSV and JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Splitting lists into columns\n",
    "\n",
    "This wasn't part of the original paper, but it's an example I run into all the time and I haven't seen it documented very many places.\n",
    "\n",
    "The data is in a sub-folder called `data`. The `read_excel()` function will read the first sheet in the workbook if you don't specify another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = pd.read_excel('./data/PeopleStates.xlsx')\n",
    "ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referring to columns with quoted name in side square brackets\n",
    "\n",
    "We can refer to a specific column either with square brackets with the name in quotes (which is the necessary form if the column name has spaces in it). Each column by itself, taken out of the DataFrame, is not a DataFrame, but a \"Series\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referring to columns with dataframe.name\n",
    "\n",
    "If the name doesn't have spaces in it, we can use the \"dot notation\", with the dataframe variable name \".\" column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String `.str` operations will be applied to each row\n",
    "\n",
    "Here we do a \"splitting\" operation on the column to split what is currently a single string containing commas, into a list of the items between the commas.\n",
    "\n",
    "*Note, you will end up with a single column of lists if don't put `expand=True`, which denotes that you're intending to \"expand the dimensionality\" of the data set.*\n",
    "\n",
    "*Notice, also, that the DataFrame will expand to enough columns to accomodate the list with the most elements, unless you specify a limit, and lists without enough elements will have `None` in the extra columns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psplit = ps.states.str.split(',', expand=True)\n",
    "psplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation â€“ `concat()`\n",
    "\n",
    "Pandas will use the Index to align rows of the original `names` Series and the `psplit` DataFrame that are being concatenated. \n",
    "\n",
    "- `axis=0` is down the rows\n",
    "- `axis=1` is across the columns.\n",
    "\n",
    "Let's put the expanded states and the names back together into one table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pexp = pd.concat([ps.name, psplit], axis=1)\n",
    "pexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Column headers are values, not variable names\n",
    "\n",
    "*One of the more common manipulations*\n",
    "\n",
    "### Un-pivoting into tall format\n",
    "\n",
    "Many call this process of going from a wide data set to tall \"un-pivoting\" since a pivot table in Excel converts data from the tall format into wide. \n",
    "\n",
    "The situation when you need this is that you have data in the column headers that you want in their own column. You also want the values that are spread across the multiple rows and columns to end up in a single measurement column.\n",
    "\n",
    "- **In Pandas you do a \"melt\"**\n",
    "- In `tidyr` this is a \"gather\"\n",
    "- In OpenRefine it's a \"Transpose->Transpose cells across columns into rows...\" operation\n",
    "- In Tableau this is called a \"Pivot\"\n",
    "\n",
    "Let's first define a simple, small data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'label':['A','B','C'],\n",
    "                  'x':[1,2,3],\n",
    "                  'y':[4,5,6],\n",
    "                  'z':[7,8,9]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimally, you need to specify the DataFrame to \"melt\", and a list of which columns don't get \"un-pivoted\". The latter will get repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.melt(df, ['label'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now back to the States dataset\n",
    "\n",
    "- id_vars will be repeated and not un-pivoted\n",
    "- all others will be melted down into a single column (values)\n",
    "- with the column names as a separate column (variables)\n",
    "\n",
    "When we don't specify a `var_name=` for `melt()`, it will default to \"variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptidy = pd.melt(pexp, id_vars=['name'], value_name='state')\n",
    "ptidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns\n",
    "\n",
    "In this case we don't need the \"variable\" column. We can specify a list of column names to select only certain columns, dropping others that aren't needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptidy = ptidy[['name','state']]\n",
    "ptidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: \"inplace\"\n",
    "\n",
    "- Most functions create a copy of the DataFrame instead of changing the original\n",
    "- Many methods include an \"inplace\" argument, so it won't make a copy\n",
    "- **Be careful! You're writing over your data in place!**\n",
    "- `dropna()` defaults to dropping any row that has a null/None in any column. You can specify a subset of colunns to look in instead.\n",
    "\n",
    "#### `dropna()` to drop nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptidy.dropna(inplace=True)\n",
    "ptidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sort_values()` to sort by values\n",
    "\n",
    "Again, the default is to make a copy, so you either have to reassign, or change \"inplace\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptidy.sort_values(by='name', inplace=True)\n",
    "ptidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Merging (joining) two data sets\n",
    "\n",
    "Here we'll read in a second sheet out of the same Excel workbook and join this state-level data with the people/states data we just modified. \n",
    "\n",
    "**This is the Pandas equivalent of an SQL JOIN command**\n",
    "\n",
    "We'll start by loading in a table of the US states, their populations, and the number of congessional house seats they are represented by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = pd.read_excel('./data/PeopleStates.xlsx', sheet_name='Sheet2')\n",
    "sp.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a LEFT JOIN by using the `merge()` function, specifying which DataFrame is on the \"left\" and which is on the \"right\" for the JOIN. It's just the order in which you list them as the first two arguments to `merge()`.\n",
    "\n",
    "We also need to specify which column contains the ID fields / keys to join on. We put these in the \"left_on\" and \"right_on\" arguments.\n",
    "\n",
    "Then, we'll sort the rows \"descending\" and \"in place\" by the populatin column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppop = pd.merge(ptidy, sp, how='left', left_on='state', right_on='state')\n",
    "\n",
    "ppop.sort_values('population_2010', ascending=False, inplace=True)\n",
    "ppop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Saving table out to a CSV file\n",
    "\n",
    "Usually we can save to an Excel file, but we'd need to install another module\n",
    "so, we'll save as CSV file for now, which is a very useful format.\n",
    "\n",
    "- It's good practice to specify the `encoding`, which is the method used for recording characters beyond the 256 ASCII character set. \n",
    "\n",
    "- In this case we also don't need to save the `index` column to the file, so we'll turn that option off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppop.to_csv('./data/PeopleStates_Merged.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to JSON\n",
    "\n",
    "Another option is to save as a JSON file. There are multiple \"orientations\":\n",
    "[to_json docs](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html)\n",
    "\n",
    "`records` orientation will make a list of rows, each an object/dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppop.to_json('./data/PeopleStates_Merged.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stop here and try the Pew Research Center Dataset exercise!\n",
    "\n",
    "**Click here to open:** [10_PewExercise.ipynb](10_PewExercise.ipynb)\n",
    "\n",
    "*Don't look yet, but solutions are in:* [11_PewExerciseSolutions.ipynb](11_PewExerciseSolutions.ipynb)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
